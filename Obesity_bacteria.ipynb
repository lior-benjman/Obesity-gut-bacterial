{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c60bf0a",
   "metadata": {},
   "source": [
    "# Imports and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9258a328",
   "metadata": {},
   "source": [
    "### Microbiome-based Obesity Analysis <br> Statistical Association and Predictive Modeling\n",
    "\n",
    "In this notebook, we analyze gut microbiome data in order to study the relationship between bacterial taxa and obesity.\n",
    "\n",
    "The goals of this analysis are twofold:\n",
    "1. **Statistical association**: identify bacterial taxa whose abundances differ significantly between lean and obese individuals.\n",
    "2. **Predictive modeling**: evaluate whether gut microbiome composition can be used to predict obesity status.\n",
    "\n",
    "The dataset was obtained from the *LeChatelierE_2013* study via the curatedMetagenomicData framework.  \n",
    "Initial data acquisition, filtering, and preprocessing were performed in a separate notebook and are summarized below.\n",
    "\n",
    "Our Final Research Question: <br>Which gut bacterial taxa are associated with obesity (BMI â‰¥ 30 vs BMI â‰¤ 25), and how well can gut microbiome composition predict obesity status?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ea87c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'MIPMLP'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMIPMLP\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preprocess\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mannwhitneyu\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmultitest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multipletests\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'MIPMLP'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import MIPMLP\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from MIPMLP import preprocess\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_auc_score, roc_curve, auc\n",
    ")\n",
    "\n",
    "# Set visual style\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f0a8e",
   "metadata": {},
   "source": [
    "# 1. Data Loading & The \"Data Challenge\"\n",
    "\n",
    "In this section, we load the raw abundance data and metadata, then define our target groups (Lean vs. Obese). We also explore the unique challenges posed by microbiome data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f958a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Metadata and Abundance data\n",
    "meta_raw = pd.read_csv('Data/Raw_LeChatelier_metadata.csv', index_col=0)\n",
    "X_raw = pd.read_csv('Data/Raw_LeChatelier_relative_abundance.csv', index_col=0)\n",
    "\n",
    "print(f\"Metadata shape: {meta_raw.shape}\")\n",
    "print(f\"Abundance matrix shape: {X_raw.shape}\")\n",
    "\n",
    "# Filter for Lean (BMI <= 25) and Obese (BMI >= 30)\n",
    "meta_filtered = meta_raw[(meta_raw['BMI'] <= 25) | (meta_raw['BMI'] >= 30)].copy()\n",
    "meta_filtered['group'] = np.where(meta_filtered['BMI'] <= 25, 'Lean', 'Obese')\n",
    "\n",
    "# Align abundance data with filtered metadata\n",
    "X = X_raw.loc[meta_filtered.index]\n",
    "y = meta_filtered['group']\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {X.shape}\")\n",
    "print(f\"Class distribution:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing_title",
   "metadata": {},
   "source": [
    "## 1.1 Data Preprocessing using MIPMLP\n",
    "\n",
    "We use the course's official pipeline (MIPMLP) for data preprocessing. This pipeline includes:\n",
    "1. **Taxonomy Aggregation**: Ensuring data is at the Species level (Level 7).\n",
    "2. **Normalization**: Applying Relative Normalization.\n",
    "3. **Filtering**: Removing rare taxa that appear in fewer than 1% of samples.\n",
    "\n",
    "*Note: We perform Log Transformation explicitly in downstream steps (visualizations and modeling) as per the pipeline's flexibility.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filtering_taxa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X for MIPMLP (Requires 'ID' column and 'taxonomy' row)\n",
    "X_mip = X.copy()\n",
    "X_mip['ID'] = X_mip.index\n",
    "X_mip = X_mip.astype(object)\n",
    "X_mip.loc['taxonomy'] = pd.Series(X_mip.columns, index=X_mip.columns)\n",
    "X_mip.loc['taxonomy', 'ID'] = 'taxonomy'\n",
    "\n",
    "# Run MIPMLP Preprocess\n",
    "# Using rare_bacteria_threshold=0.01 (1%) as per default tool settings\n",
    "# Using normalization='relative' to keep data relative for subsequent log-transform in ML/Plots\n",
    "X_processed = preprocess(\n",
    "    X_mip, \n",
    "    taxonomy_level=7, \n",
    "    taxnomy_group='sum', \n",
    "    normalization='relative', \n",
    "    rare_bacteria_threshold=0.01,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "# Update X with processed data\n",
    "X = X_processed\n",
    "\n",
    "print(f\"Original shape: {X_raw.shape}\")\n",
    "print(f\"Processed shape (after MIPMLP filtering): {X.shape}\")\n",
    "\n",
    "# Re-calculate sparsity\n",
    "new_sparsity = (X == 0).sum().sum() / X.size\n",
    "print(f\"New Global Sparsity: {new_sparsity:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenge_markdown",
   "metadata": {},
   "source": [
    "### The Microbiome Data Challenge\n",
    "Working with gut microbiome data presents several statistical and computational challenges:\n",
    "1. **High Dimensionality:** We often have hundreds or thousands of bacterial taxa (features) for a relatively small number of patients.\n",
    "2. **Sparsity:** Microbiome matrices are \"zero-heavy\" because most taxa are not present in every individual.\n",
    "3. **Compositionality & Skewness:** Abundance values are relative (sum to 100%) and often follow a heavy-tailed distribution, where a few taxa dominate and many others are rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_sparsity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1: Sparsity\n",
    "sparsity = (X == 0).sum().sum() / X.size\n",
    "print(f\"Global Sparsity: {sparsity:.2%}\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "nonzero_counts = (X > 0).sum(axis=1)\n",
    "sns.histplot(nonzero_counts, kde=True, color='teal')\n",
    "plt.title(\"Challenge: Sparsity (Number of observed Taxa per Sample)\")\n",
    "plt.xlabel(\"Number of Non-zero Taxa\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_imbalance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 2: Class Imbalance\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=y, palette=['#66c2a5', '#fc8d62'])\n",
    "plt.title(\"Challenge: Class Imbalance\")\n",
    "plt.ylabel(\"Number of Subjects\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_skewness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 3: Skewness & The need for Log Transformation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sample_taxon = X.columns[0] # Pick the first taxon as an example\n",
    "\n",
    "sns.histplot(x=X[sample_taxon], hue=y, bins=30, ax=axes[0], palette=['#66c2a5', '#fc8d62'], element=\"step\", common_norm=False)\n",
    "axes[0].set_title(f\"Raw Abundance Distribution\\n({sample_taxon.split('|')[-1]})\")\n",
    "axes[0].set_xlabel(\"Relative Abundance\")\n",
    "\n",
    "# Apply Log Transformation (with pseudo-count)\n",
    "X_log = np.log10(X + 1e-6)\n",
    "\n",
    "sns.histplot(x=X_log[sample_taxon], hue=y, bins=30, ax=axes[1], palette=['#66c2a5', '#fc8d62'], element=\"step\", common_norm=False)\n",
    "axes[1].set_title(f\"Log-Transformed Distribution\")\n",
    "axes[1].set_xlabel(\"Log10(Relative Abundance)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "goldilocks_hypothesis",
   "metadata": {},
   "source": [
    "### Hypothesis Check: Is \"Balance\" the key?\n",
    "\n",
    "We observed that looking at a random taxon might not show a clear signal. Let's find the **most significant taxon** in our dataset (the one with the lowest p-value between Lean and Obese) and test the \"Goldilocks Hypothesis\" on it.\n",
    "\n",
    "We categorize its abundance into:\n",
    "1. **Absent**: 0 abundance\n",
    "2. **Intermediate**: > 0 but below the median of non-zero values\n",
    "3. **High**: Above the median of non-zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_goldilocks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most significant taxon first\n",
    "p_values = {}\n",
    "for taxon in X.columns:\n",
    "    group1 = X.loc[y == 'Lean', taxon]\n",
    "    group2 = X.loc[y == 'Obese', taxon]\n",
    "    _, p = mannwhitneyu(group1, group2)\n",
    "    p_values[taxon] = p\n",
    "\n",
    "best_taxon = min(p_values, key=p_values.get)\n",
    "print(f\"Switching analysis to most significant taxon: {best_taxon.split('|')[-1]} (p={p_values[best_taxon]:.2e})\")\n",
    "\n",
    "# Define thresholds based on non-zero data\n",
    "taxon_data = X[best_taxon]\n",
    "nonzero_data = taxon_data[taxon_data > 0]\n",
    "median_nonzero = nonzero_data.median()\n",
    "\n",
    "# Categorize\n",
    "conditions = [\n",
    "    (taxon_data == 0),\n",
    "    (taxon_data > 0) & (taxon_data <= median_nonzero),\n",
    "    (taxon_data > median_nonzero)\n",
    "]\n",
    "categories = ['Absent', 'Intermediate', 'High']\n",
    "df_cat = pd.DataFrame({'Category': np.select(conditions, categories, default='Unknown'), 'Group': y})\n",
    "\n",
    "# Calculate proportions\n",
    "prop_data = df_cat.groupby(['Category', 'Group']).size().reset_index(name='Count')\n",
    "total_per_cat = prop_data.groupby('Category')['Count'].transform('sum')\n",
    "prop_data['Percentage'] = prop_data['Count'] / total_per_cat * 100\n",
    "\n",
    "# Order categories logic\n",
    "category_order = ['Absent', 'Intermediate', 'High']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=prop_data, x='Category', y='Percentage', hue='Group', \n",
    "            order=category_order, palette=['#66c2a5', '#fc8d62'])\n",
    "plt.title(f\"Obesity Prevalence by {best_taxon.split('|')[-1]} Abundance\")\n",
    "plt.ylabel(\"Percentage of Individuals (%)\")\n",
    "plt.xlabel(\"Bacterial Abundance Category\")\n",
    "plt.legend(title='Group')\n",
    "plt.show()\n",
    "\n",
    "print(\"Group distribution per category:\")\n",
    "print(pd.crosstab(df_cat['Category'], df_cat['Group'], normalize='index').round(3) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "goldilocks_conclusion",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border-radius: 10px; background-color: #f0f7f4; border-left: 5px solid #66c2a5;\">\n",
    "    <h3 style=\"color: #1e5631; margin: 0;\">ðŸ”¬ Conclusion: High Abundance is Protective</h3>\n",
    "    <p style=\"color: #34495e; margin: 10px 0;\">\n",
    "        Contrary to the \"Balance\" hypothesis, <b>High Abundance</b> of <i>Fretibacterium fastidiosum</i> correlates with lower obesity risk.\n",
    "    </p>\n",
    "    <ul style=\"color: #7f8c8d; margin-bottom: 0;\">\n",
    "        <li><b style=\"color: #d35400;\">Low/Absent:</b> ~80% Obese (High Risk).</li>\n",
    "        <li><b style=\"color: #27ae60;\">High:</b> ~50% Lean (Doubled chance of being Lean).</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical_analysis_title",
   "metadata": {},
   "source": [
    "# 2. Statistical Association Analysis\n",
    "\n",
    "We use the non-parametric Mann-Whitney U test to identify taxa that differ significantly between Lean and Obese individuals. Since we are testing hundreds of taxa, we must apply a Multiple Testing Correction (Benjamini-Hochberg FDR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mann_whitney_tests",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into two groups\n",
    "X_lean = X[y == 'Lean']\n",
    "X_obese = X[y == 'Obese']\n",
    "\n",
    "results = []\n",
    "for taxon in X.columns:\n",
    "    # Perform Mann-Whitney U test\n",
    "    stat, pval = mannwhitneyu(X_lean[taxon], X_obese[taxon], alternative='two-sided')\n",
    "    \n",
    "    # Calculate mean abundance for each group\n",
    "    mean_lean = X_lean[taxon].mean()\n",
    "    mean_obese = X_obese[taxon].mean()\n",
    "    \n",
    "    # Calculate Log2 Fold Change (adding a small epsilon to avoid log(0))\n",
    "    # Note: Using means of relative abundances\n",
    "    l2fc = np.log2((mean_obese + 1e-6) / (mean_lean + 1e-6))\n",
    "    \n",
    "    results.append({\n",
    "        'taxon': taxon,\n",
    "        'p_value': pval,\n",
    "        'mean_lean': mean_lean,\n",
    "        'mean_obese': mean_obese,\n",
    "        'log2fc': l2fc\n",
    "    })\n",
    "\n",
    "df_stats = pd.DataFrame(results)\n",
    "\n",
    "# Apply Benjamini-Hochberg FDR correction\n",
    "_, df_stats['fdr_q_value'], _, _ = multipletests(df_stats['p_value'], method='fdr_bh')\n",
    "\n",
    "# Filter for significant taxa (FDR < 0.1 as a common discovery threshold in microbiome)\n",
    "significant_taxa = df_stats[df_stats['fdr_q_value'] < 0.1].sort_values('fdr_q_value')\n",
    "\n",
    "print(f\"Found {len(significant_taxa)} significant taxa at FDR < 0.1\")\n",
    "significant_taxa.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "volcano_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volcano Plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "df_stats['neg_log10_p'] = -np.log10(df_stats['p_value'])\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df_stats, x='log2fc', y='neg_log10_p', \n",
    "    hue=df_stats['fdr_q_value'] < 0.1, \n",
    "    palette={True: 'crimson', False: 'lightgrey'}, alpha=0.6\n",
    ")\n",
    "\n",
    "plt.axhline(-np.log10(0.05), color='black', linestyle='--', alpha=0.5)\n",
    "plt.title(\"Volcano Plot: Taxa associated with Obesity\")\n",
    "plt.xlabel(\"Log2 Fold Change (Obese / Lean)\")\n",
    "plt.ylabel(\"-Log10 p-value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predictive_modeling_title",
   "metadata": {},
   "source": [
    "# 3. Predictive Modeling\n",
    "\n",
    "Can we predict if a person is Lean or Obese based on their microbiome? We use Logistic Regression with L1 regularization (Lasso) to both predict and identify the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ml_pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target\n",
    "y_numeric = (y == 'Obese').astype(int)\n",
    "\n",
    "# Log transform X (as seen in Challenge 3)\n",
    "X_ml = np.log10(X + 1e-6)\n",
    "\n",
    "# Build Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(penalty='l1', solver='liblinear', C=1.0, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Evaluation with Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "aucs = []\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X_ml, y_numeric)):\n",
    "    X_train, X_test = X_ml.iloc[train_index], X_ml.iloc[test_index]\n",
    "    y_train, y_test = y_numeric.iloc[train_index], y_numeric.iloc[test_index]\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_score = pipe.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    \n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label=f'ROC fold {i} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=0.8)\n",
    "plt.title(f'Cross-Validated ROC Curve\\nMean AUC = {np.mean(aucs):.2f}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average AUC: {np.mean(aucs):.2f} (+/- {np.std(aucs):.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_importance_title",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Which taxa were most important for the model's prediction? We can look at the coefficients of the L1-regularized Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on full dataset to get final coefficients\n",
    "pipe.fit(X_ml, y_numeric)\n",
    "coefs = pipe.named_steps['clf'].coef_[0]\n",
    "feature_names = X.columns\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'taxon': feature_names,\n",
    "    'coefficient': coefs\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "# Filter for non-zero coefficients\n",
    "importance_df = importance_df[importance_df['coefficient'] != 0]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(\n",
    "    data=importance_df.head(20), \n",
    "    x='coefficient', y='taxon', palette='vlag'\n",
    ")\n",
    "plt.title(\"Top 20 Predictive Taxa (Logistic Regression Coefficients)\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Number of features selected by Lasso: {len(importance_df)}\")\n",
    "importance_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
